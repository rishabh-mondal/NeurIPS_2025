{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import supervision as sv\n",
    "from supervision.metrics import MeanAveragePrecision\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "classes= ['CFCBK', 'FCBK', 'Zigzag']\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "config=\".py\"\n",
    "# Path to your config file\n",
    "config_file_path = f\"../mmrotate_brickkiln/configs/{config}.py\"\n",
    "\n",
    "# Read config file\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config_text = file.read()\n",
    "\n",
    "# Extract all `type=...` assignments (handles both single and double quotes)\n",
    "type_matches = re.findall(r\"type\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]\", config_text)\n",
    "\n",
    "# Remove duplicates (optional)\n",
    "type_matches = list(set(type_matches))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(type_matches, columns=[\"type\"])\n",
    "\n",
    "# Show or save\n",
    "print(df)\n",
    "# df.to_csv(\"config_types.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_names_from_directory(directory):\n",
    "    \"\"\"Extracts image names (without extension) from a directory.\"\"\"\n",
    "    return {file_name.replace(\".txt\", \"\") for file_name in os.listdir(directory) if file_name.endswith(\".txt\")}\n",
    "\n",
    "def load_detections(annotations_path, img_names, is_gt=True, confidence_threshold=0):\n",
    "    \"\"\"Loads detections only for images that exist in both GT and Predictions.\"\"\"\n",
    "    sv_data = []\n",
    "\n",
    "    for image_id in sorted(img_names):\n",
    "        file_path = os.path.join(annotations_path, f\"{image_id}.txt\")\n",
    "        if not os.path.exists(file_path):  # Ensure file exists before processing\n",
    "            continue\n",
    "\n",
    "        xyxy_list = []\n",
    "        class_ids = []\n",
    "        scores = []\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        if not lines:\n",
    "            detection = sv.Detections(\n",
    "                xyxy=np.empty((0, 4)),\n",
    "                class_id=np.empty((0,)),\n",
    "                confidence=np.empty((0,)),\n",
    "                # metadata={\"image_id\": image_id}\n",
    "            )\n",
    "            sv_data.append(detection)\n",
    "            continue\n",
    "        \n",
    "        for line in lines:\n",
    "            data = line.strip().split(\" \")\n",
    "            class_name = data[8]\n",
    "            class_id = classes.index(class_name) if class_name in classes else -1\n",
    "            if class_id == -1:\n",
    "                continue\n",
    "            polygon = np.array(list(map(float, data[:8]))).reshape(4, 2)  # Convert to (4,2) shape with floats\n",
    "            score = float(data[9]) if not is_gt else 1.0  # Default confidence for GT is 1.0\n",
    "\n",
    "            if not is_gt and score < confidence_threshold:\n",
    "                continue\n",
    "\n",
    "            # Convert quadrilateral to bounding box (min x, min y, max x, max y)\n",
    "            x_min, y_min = np.min(polygon, axis=0)\n",
    "            x_max, y_max = np.max(polygon, axis=0)\n",
    "            bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "            # Append to lists\n",
    "            xyxy_list.append(bbox)\n",
    "            class_ids.append(class_id)\n",
    "            scores.append(score)\n",
    "\n",
    "        # Convert lists into a Supervision Detections object\n",
    "        detections = sv.Detections(\n",
    "            xyxy=np.array(xyxy_list),\n",
    "            class_id=np.array(class_ids),\n",
    "            confidence=np.array(scores),\n",
    "            # metadata={\"image_id\": image_id}\n",
    "        )\n",
    "\n",
    "        sv_data.append(detections)\n",
    "\n",
    "    return sv_data\n",
    "\n",
    "def get_class_counts(detections_list, num_classes=3):\n",
    "    \"\"\"Counts occurrences of each class in ground truth detections.\"\"\"\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    for detections in detections_list:\n",
    "        unique, counts = np.unique(detections.class_id, return_counts=True)\n",
    "        for cls, count in zip(unique, counts):\n",
    "            # print(cls, count)\n",
    "            class_counts[cls] += count\n",
    "    return class_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# .py\n",
    "def extract_model_structure(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    return {\n",
    "        \"model_type\": re.search(r\"model\\s*=\\s*dict\\([\\s\\S]*?type=['\\\"](.+?)['\\\"]\", content).group(1) if re.search(r\"model\\s*=\", content) else None,\n",
    "        \"backbone_type\": re.search(r\"backbone\\s*=\\s*dict\\([\\s\\S]*?type=['\\\"](.+?)['\\\"]\", content).group(1) if re.search(r\"backbone\\s*=\", content) else None,\n",
    "        \"neck_type\": re.search(r\"neck\\s*=\\s*dict\\([\\s\\S]*?type=['\\\"](.+?)['\\\"]\", content).group(1) if re.search(r\"neck\\s*=\", content) else None,\n",
    "        \"bbox_head_type\": re.search(r\"bbox_head\\s*=\\s*dict\\([\\s\\S]*?type=['\\\"](.+?)['\\\"]\", content).group(1) if re.search(r\"bbox_head\\s*=\", content) else None,\n",
    "    }\n",
    "\n",
    "# # === CONFIGURATIONS === #\n",
    "# Base paths\n",
    "base_path = \n",
    "data_base_path = \n",
    "data_type = 'test'\n",
    "inference_dir = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/jeet/test/results\"\n",
    "working_dir_name = \"work_dirs\"\n",
    "\n",
    "# Define config_name separately to use it in config_file and inference_dir\n",
    "config_name = ''\n",
    "model_name = ''\n",
    "\n",
    "# Path to the config file\n",
    "# config_file_path = os.path.join(base_path, f\"configs/{model_name}/{config_name}.py\")\n",
    "\n",
    "print(f\"Config file path: {config_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "model_info = extract_model_structure(config_file_path)\n",
    "\n",
    "model_configs = [\n",
    "    {\n",
    "        'model_name': model_name,\n",
    "        'train_data': \"stratified_train\",\n",
    "        'test_data': \"stratified_test\",\n",
    "        'config_name': config_name,\n",
    "        'config_file': config_file_path,\n",
    "        # 'checkpoint_folder': os.path.join(base_path, f'{working_dir_name}/{model_name}'),\n",
    "        'val_dir': os.path.join(data_base_path, 'test'),\n",
    "        'inference_dir': os.path.join(inference_dir, f\"test_{config_name}_nms_0.5_conf_0.1\"),\n",
    "        'epoch': 25,\n",
    "        'conf_threshold': 0.05,\n",
    "        **model_info\n",
    "    }\n",
    "]\n",
    "\n",
    "# Validate paths\n",
    "for cfg in model_configs:\n",
    "    assert os.path.isfile(cfg['config_file']), f\"Missing config file: {cfg['config_file']}\"\n",
    "    # assert os.path.isdir(cfg['checkpoint_folder']), f\"Missing checkpoint folder: {cfg['checkpoint_folder']}\"\n",
    "    assert os.path.isdir(cfg['val_dir']), f\"Missing validation directory: {cfg['val_dir']}\"\n",
    "\n",
    "# === RESULTS DF SETUP === #\n",
    "columns = ['model_name', \"CFCBK\", \"FCBK\", \"Zigzag\", \"Weighted mAP@50\", \"mAP@50:95\", \"mAP@50\", \"mAP@75\",\n",
    "           \"CA mAP@50:95\", \"CA mAP@50\", \"CA mAP@75\", \"confidence_threshold\",\n",
    "           \"backbone\", \"head\", \"epoch\", \"model_type\", \"backbone_type\", \"neck_type\", \"bbox_head_type\"]\n",
    "\n",
    "index = pd.MultiIndex.from_tuples([], names=[\"Backbone\", \"Head\", \"Epoch\"])\n",
    "result_df = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "# === EVALUATION === #\n",
    "for cfg in model_configs:\n",
    "    print(f\"\\nEvaluating: {cfg['config_name']}\")\n",
    "\n",
    "    GT_PATH = os.path.join(cfg['val_dir'], \"labelTxt\")\n",
    "    print(f\"GT_PATH: {GT_PATH}\")\n",
    "    # PRED_PATH = os.path.join(cfg['inference_dir'], f\"epoch_{cfg['epoch']}\", \"annfiles\")\n",
    "    # print(f\"PRED_PATH: {PRED_PATH}\")\n",
    "    PRED_PATH=\"/annfiles\"\n",
    "\n",
    "    assert os.path.isdir(GT_PATH), f\"Missing GT path: {GT_PATH}\"\n",
    "    assert os.path.isdir(PRED_PATH), f\"Missing Predictions: {PRED_PATH}\"\n",
    "\n",
    "    gt_imgs = get_image_names_from_directory(GT_PATH)\n",
    "    pred_imgs = get_image_names_from_directory(PRED_PATH)\n",
    "    common_imgs = gt_imgs.intersection(pred_imgs)\n",
    "    assert len(common_imgs) > 0, \"No common images to evaluate!\"\n",
    "\n",
    "    gt_data = load_detections(GT_PATH, common_imgs, is_gt=True)\n",
    "    pred_data = load_detections(PRED_PATH, common_imgs, is_gt=False, confidence_threshold=cfg['conf_threshold'])\n",
    "\n",
    "    # Class-wise mAP\n",
    "    mAP = MeanAveragePrecision(class_agnostic=False)\n",
    "    mAP_result = mAP.update(pred_data, gt_data).compute()\n",
    "\n",
    "    matched = mAP_result.matched_classes.tolist()\n",
    "    class_ap = mAP_result.ap_per_class[:, 0].tolist()\n",
    "    class_ap_final = [0.0] * 3\n",
    "    for cls, ap in zip(matched, class_ap):\n",
    "        class_ap_final[cls] = ap\n",
    "\n",
    "    counts = get_class_counts(gt_data, num_classes=3)\n",
    "    weighted_ap = np.sum(np.array(class_ap_final) * counts) / np.sum(counts)\n",
    "\n",
    "    # Class-agnostic mAP\n",
    "    mAP_ca = MeanAveragePrecision(class_agnostic=True)\n",
    "    mAP_result_ca = mAP_ca.update(pred_data, gt_data).compute()\n",
    "\n",
    "    row_key = (cfg['backbone_type'], cfg['bbox_head_type'], cfg['epoch'])\n",
    "    row_data = class_ap_final + [\n",
    "        weighted_ap,\n",
    "        mAP_result.map50_95, mAP_result.map50, mAP_result.map75,\n",
    "        mAP_result_ca.map50_95, mAP_result_ca.map50, mAP_result_ca.map75,\n",
    "        cfg['conf_threshold'],\n",
    "        cfg['backbone_type'], cfg['bbox_head_type'], cfg['epoch'],\n",
    "        cfg['model_type'], cfg['backbone_type'], cfg['neck_type'], cfg['bbox_head_type']\n",
    "    ]\n",
    "\n",
    "    result_df.loc[row_key] = [f\"{v:.6f}\" if isinstance(v, float) else v for v in [cfg['model_name']] + row_data]\n",
    "\n",
    "#import os\n",
    "import pandas as pd\n",
    "\n",
    "# === SAVE RESULTS === #\n",
    "base_path = \"\"\n",
    "csv_path = os.path.join(base_path, \"map_results_with_model_info.csv\")\n",
    "\n",
    "# Example: result_df should already be defined before this\n",
    "# For demonstration, here's a dummy example:\n",
    "# result_df = pd.DataFrame([{\"model\": \"YOLOv8\", \"mAP\": 0.72, \"F1\": 0.85}])\n",
    "\n",
    "# Append new results to the existing CSV file\n",
    "if os.path.exists(csv_path):\n",
    "    old_df = pd.read_csv(csv_path)\n",
    "    result_df = pd.concat([old_df, result_df], ignore_index=True)\n",
    "else:\n",
    "    print(\"CSV file does not exist. A new file will be created.\")\n",
    "\n",
    "# Save the updated DataFrame\n",
    "result_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Print the DataFrame and confirmation\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(result_df)\n",
    "print(\"\\n✅ Evaluation complete. Results saved to 'map_results_with_model_info.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append to save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the columns [model_name CA mAP@50\tCFCBK\tFCBK\tZigzag\t]\n",
    "s=s[['model_name', 'CA mAP@50', 'CFCBK', 'FCBK', 'Zigzag']]\n",
    "#calculate parcentage of each column \n",
    "s['CA mAP@50'] = (s['CA mAP@50'] * 100)\n",
    "s['CFCBK'] = (s['CFCBK'] * 100)\n",
    "s['FCBK'] = (s['FCBK'] * 100)\n",
    "s['Zigzag'] = (s['Zigzag'] * 100)\n",
    "#save the dataframe to csv\n",
    "# s.to_csv(os.path.join(base_path, \"map_results_with_model_info_percentage.csv\"), index=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# csv_path = os.path.join(base_path, \"map_results_with_model_info.csv\")\n",
    "s = s[['model_name', 'CA mAP@50', 'CFCBK', 'FCBK', 'Zigzag']]\n",
    "\n",
    "# Convert values to percentages and round to 2 decimal places\n",
    "for col in ['CA mAP@50', 'CFCBK', 'FCBK', 'Zigzag']:\n",
    "    s[col] = (s[col]).round(2)\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "# s.to_csv(os.path.join(base_path, \"map_results_with_model_info_percentage.csv\"), index=False)\n",
    "\n",
    "for _, row in s.iterrows():\n",
    "    model_name = row['model_name']\n",
    "    values = [f\"{row[col]:.2f}\" for col in ['CA mAP@50', 'CFCBK', 'FCBK', 'Zigzag']]\n",
    "    formatted_row = f\"{model_name} & \" + \" & \".join(values) + \" \\\\\\\\\"\n",
    "    print(formatted_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
